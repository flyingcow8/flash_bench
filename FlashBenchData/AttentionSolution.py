# automatically generated by the FlatBuffers compiler, do not modify

# namespace: FlashBenchData

import flatbuffers
from flatbuffers.compat import import_numpy
np = import_numpy()

class AttentionSolution(object):
    __slots__ = ['_tab']

    @classmethod
    def GetRootAs(cls, buf, offset=0):
        n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
        x = AttentionSolution()
        x.Init(buf, n + offset)
        return x

    @classmethod
    def GetRootAsAttentionSolution(cls, buf, offset=0):
        """This method is deprecated. Please switch to GetRootAs."""
        return cls.GetRootAs(buf, offset)
    # AttentionSolution
    def Init(self, buf, pos):
        self._tab = flatbuffers.table.Table(buf, pos)

    # AttentionSolution
    def HeadDim(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
        if o != 0:
            return self._tab.Get(flatbuffers.number_types.Int32Flags, o + self._tab.Pos)
        return 0

    # AttentionSolution
    def GridType(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
        if o != 0:
            return self._tab.Get(flatbuffers.number_types.Int8Flags, o + self._tab.Pos)
        return 0

    # AttentionSolution
    def BalanceType(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
        if o != 0:
            return self._tab.Get(flatbuffers.number_types.Int8Flags, o + self._tab.Pos)
        return 0

    # AttentionSolution
    def NumSplits(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(10))
        if o != 0:
            return self._tab.Get(flatbuffers.number_types.Int32Flags, o + self._tab.Pos)
        return 0

    # AttentionSolution
    def KernelType(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(12))
        if o != 0:
            return self._tab.Get(flatbuffers.number_types.Int8Flags, o + self._tab.Pos)
        return 0

    # AttentionSolution
    def KernelId(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(14))
        if o != 0:
            return self._tab.String(o + self._tab.Pos)
        return None

def AttentionSolutionStart(builder):
    builder.StartObject(6)

def Start(builder):
    AttentionSolutionStart(builder)

def AttentionSolutionAddHeadDim(builder, headDim):
    builder.PrependInt32Slot(0, headDim, 0)

def AddHeadDim(builder, headDim):
    AttentionSolutionAddHeadDim(builder, headDim)

def AttentionSolutionAddGridType(builder, gridType):
    builder.PrependInt8Slot(1, gridType, 0)

def AddGridType(builder, gridType):
    AttentionSolutionAddGridType(builder, gridType)

def AttentionSolutionAddBalanceType(builder, balanceType):
    builder.PrependInt8Slot(2, balanceType, 0)

def AddBalanceType(builder, balanceType):
    AttentionSolutionAddBalanceType(builder, balanceType)

def AttentionSolutionAddNumSplits(builder, numSplits):
    builder.PrependInt32Slot(3, numSplits, 0)

def AddNumSplits(builder, numSplits):
    AttentionSolutionAddNumSplits(builder, numSplits)

def AttentionSolutionAddKernelType(builder, kernelType):
    builder.PrependInt8Slot(4, kernelType, 0)

def AddKernelType(builder, kernelType):
    AttentionSolutionAddKernelType(builder, kernelType)

def AttentionSolutionAddKernelId(builder, kernelId):
    builder.PrependUOffsetTRelativeSlot(5, flatbuffers.number_types.UOffsetTFlags.py_type(kernelId), 0)

def AddKernelId(builder, kernelId):
    AttentionSolutionAddKernelId(builder, kernelId)

def AttentionSolutionEnd(builder):
    return builder.EndObject()

def End(builder):
    return AttentionSolutionEnd(builder)
