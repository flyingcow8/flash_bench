# MHA (Multi-Head Attention) Parameters
mha_api: "flash_attn_varlen_func"

mha_params:
  - head_dim_qk: 128
    head_dim_v: 128
    num_heads_q: 56
    num_heads_kv: 56
    batch_size: 2
    seqlens_q:
      - 4096
      - 4096
    seqlens_kv:
      - 4096
      - 4096
    dtype: bfloat16
    dropout: false
    causal: true
    alibi: false
    window_left: -1
    window_right: -1
    attn_mask: false
    deterministic: false
    paged_kv: false
    is_training: true