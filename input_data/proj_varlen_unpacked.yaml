# MHA (Multi-Head Attention) Parameters
mha_api: "flash_attn_varlen_func"

mha_params:
  - head_dim: 80
    num_heads_q: 8
    num_heads_kv: 8
    batch_size: 2
    seqlens_q:
      - 1024
      - 1024
    seqlens_kv:
      - 1024
      - 1024
    dtype: bfloat16
    dropout: false
    causal: true
    alibi: false
    window_left: -1
    window_right: -1
    attn_mask: false
    deterministic: false
    paged_kv: false
    is_training: true

  - head_dim: 80
    num_heads_q: 8
    num_heads_kv: 8
    batch_size: 4
    seqlens_q:
      - 256
      - 257
      - 512
      - 1024
    seqlens_kv:
      - 256
      - 257
      - 512
      - 1024
    dtype: bfloat16
    dropout: false
    causal: true
    alibi: false
    window_left: -1
    window_right: -1
    attn_mask: false
    deterministic: false
    paged_kv: false
    is_training: true