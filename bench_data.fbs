namespace FlashBenchData;

enum GridType: byte {
  NHB = 0,
  NBH,
  HNB,
  BNH,
  BHN,
  HBN,
}

enum BalanceType: byte {
  None = 0,
  Mode1,
}

enum ApiType: byte {
  FlashAttnFunc = 0,
  FlashAttnQKVPackedFunc,
  FlashAttnKVPackedFunc,
  FlashAttnVarlenQKVPackedFunc,
  FlashAttnVarlenKVPackedFunc,
  FlashAttnVarlenFunc,
  FlashAttnWithKVCache,
}

enum KernelType: byte {
  Fwd = 0,
  FwdSplitkv,
  Bwd,
}

table AttentionSolution {
  head_dim: int32;
  block_m: int32;
  block_n: int32;
  num_warps: int32;
  head_dim_v: int32;
  grid_type: GridType;
  kernel_type: KernelType;
  kernel_id: string;
  // fwd or bwd
  balance_type: BalanceType;
  // fwd or fwd_split
  is_q_in_regs: bool;
  share_q_k_smem: bool;
  num_splits: int32;
  // bwd
  is_k_in_regs: bool;
  is_v_in_regs: bool;
  atom_layout_mdq: int32;
  atom_layout_msdp: int32;
  atom_layout_ndkv: int32;
  no_double_buffer: bool
}

enum DataType: byte {
  float16 = 0,
  bfloat16,
}

enum TAG: byte {
  Deploy = 0,
  TestFwd,
  TestBwd,
}

table AttentionProblem {
  dtype: DataType;
  head_dim: int32;
  num_heads_q: int32;
  num_heads_kv: int32;
  batch_size: int32;
  seqlens_q: [int32];
  seqlens_kv: [int32];
  causal: bool;
  dropout: bool;
  alibi: bool;
  local: bool;
  attn_mask: bool;
  deterministic: bool;
  paged_kv: bool;
  paged_block_size: int32;
  paged_num_blocks: int32;
  append_kv: bool;
  api: ApiType;
  is_training: bool;
  solution_fwd: AttentionSolution;
  solution_bwd: AttentionSolution;
}

table AttentionBenchTable {
  problems: [AttentionProblem];
  tag: TAG;
  version: string;
}

root_type AttentionBenchTable;
